{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b8ce53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16050 images belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 637ms/step - accuracy: 0.4900 - loss: 1.3164\n",
      "Epoch 2/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 386ms/step - accuracy: 0.6919 - loss: 0.8206\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 612ms/step - accuracy: 0.7625 - loss: 0.6542\n",
      "Epoch 4/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 389ms/step - accuracy: 0.7881 - loss: 0.5739\n",
      "Epoch 5/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 617ms/step - accuracy: 0.8268 - loss: 0.4525\n",
      "Epoch 6/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 379ms/step - accuracy: 0.8411 - loss: 0.4420\n",
      "Epoch 7/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 653ms/step - accuracy: 0.8970 - loss: 0.2919\n",
      "Epoch 8/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 789ms/step - accuracy: 0.8864 - loss: 0.3228\n",
      "Epoch 9/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 1s/step - accuracy: 0.9253 - loss: 0.1965\n",
      "Epoch 10/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 744ms/step - accuracy: 0.9139 - loss: 0.2303\n",
      "Epoch 11/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 1s/step - accuracy: 0.9402 - loss: 0.1673\n",
      "Epoch 12/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 739ms/step - accuracy: 0.9459 - loss: 0.1488\n",
      "Epoch 13/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 1s/step - accuracy: 0.9633 - loss: 0.1081\n",
      "Epoch 14/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 729ms/step - accuracy: 0.9584 - loss: 0.1148\n",
      "Epoch 15/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 1s/step - accuracy: 0.9734 - loss: 0.0769\n",
      "Epoch 16/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 735ms/step - accuracy: 0.9646 - loss: 0.1069\n",
      "Epoch 17/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 1s/step - accuracy: 0.9805 - loss: 0.0604\n",
      "Epoch 18/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 736ms/step - accuracy: 0.9724 - loss: 0.0861\n",
      "Epoch 19/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 1s/step - accuracy: 0.9816 - loss: 0.0484\n",
      "Epoch 20/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 678ms/step - accuracy: 0.9784 - loss: 0.0666\n",
      "Epoch 21/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 1s/step - accuracy: 0.9858 - loss: 0.0448\n",
      "Epoch 22/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 735ms/step - accuracy: 0.9898 - loss: 0.0369\n",
      "Epoch 23/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 1s/step - accuracy: 0.9856 - loss: 0.0428\n",
      "Epoch 24/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 741ms/step - accuracy: 0.9707 - loss: 0.0905\n",
      "Epoch 25/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 1s/step - accuracy: 0.9909 - loss: 0.0298\n",
      "Epoch 26/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 735ms/step - accuracy: 0.9900 - loss: 0.0347\n",
      "Epoch 27/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 1s/step - accuracy: 0.9858 - loss: 0.0446\n",
      "Epoch 28/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 745ms/step - accuracy: 0.9800 - loss: 0.0619\n",
      "Epoch 29/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 1s/step - accuracy: 0.9856 - loss: 0.0486\n",
      "Epoch 30/30\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 738ms/step - accuracy: 0.9920 - loss: 0.0267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4110 images belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 243ms/step - accuracy: 0.7722 - loss: 1.9722\n",
      "Test loss: 1.8118077516555786\n",
      "Test accuracy: 0.7710462212562561\n"
     ]
    }
   ],
   "source": [
    "    import os\n",
    "    import numpy as np\n",
    "    from tensorflow.keras.models import Sequential, load_model\n",
    "    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GRU, TimeDistributed\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "    # Set the path to your dataset\n",
    "    train_data_dir = r\"D:\\new smote\\train-20230326T152931Z-001\\train\"\n",
    "\n",
    "    # Specify image dimensions and batch size\n",
    "    img_width, img_height = 150, 150\n",
    "    batch_size = 32\n",
    "\n",
    "    # Extract class names from the folder names\n",
    "    class_names = sorted(os.listdir(train_data_dir))\n",
    "\n",
    "    # Create data generator for training data\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='sparse',\n",
    "        shuffle=False)  # Disable shuffling to maintain the order of images\n",
    "\n",
    "    # Reshape the input data to include a time dimension\n",
    "    X_train_reshaped = []\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(len(train_generator)):\n",
    "        images, labels = train_generator[i]\n",
    "        for j in range(len(images)):\n",
    "            X_train_reshaped.append(images[j])\n",
    "            y_train.append(labels[j])\n",
    "\n",
    "    X_train_reshaped = np.array(X_train_reshaped)\n",
    "    y_train = np.array(y_train, dtype=np.int32)\n",
    "\n",
    "    # Add a time dimension to the input data\n",
    "    X_train_reshaped = np.expand_dims(X_train_reshaped, axis=1)\n",
    "\n",
    "    # Define the GRU model\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(1, img_width, img_height, 3)))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "    model.add(TimeDistributed(Conv2D(128, (3, 3), activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(GRU(128, activation='relu', return_sequences=False))\n",
    "    model.add(Dense(len(class_names), activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train_reshaped,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=30,\n",
    "        steps_per_epoch=312)\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save('lung_detection_grunot30.h5')\n",
    "\n",
    "    # Set the path to your test dataset\n",
    "    test_data_dir = r\"D:\\new smote\\test-20230326T155708Z-001\\test\"\n",
    "\n",
    "    # Create data generator for test data\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='sparse',\n",
    "        shuffle=False)  # Disable shuffling to maintain the order of images\n",
    "\n",
    "    # Reshape the input data to include a time dimension\n",
    "    X_test_reshaped = []\n",
    "    y_test = []\n",
    "\n",
    "    for i in range(len(test_generator)):\n",
    "        images, labels = test_generator[i]\n",
    "        for j in range(len(images)):\n",
    "            X_test_reshaped.append(images[j])\n",
    "            y_test.append(labels[j])\n",
    "\n",
    "    X_test_reshaped = np.array(X_test_reshaped)\n",
    "    y_test = np.array(y_test, dtype=np.int32)\n",
    "\n",
    "    # Add a time dimension to the input data\n",
    "    X_test_reshaped = np.expand_dims(X_test_reshaped, axis=1)\n",
    "\n",
    "    # Load the trained model\n",
    "    model = load_model('lung_detection_grunot30.h5')\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "    print(f\"Test loss: {loss}\")\n",
    "    print(f\"Test accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "614bfea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 270ms/step - accuracy: 0.7722 - loss: 1.9722\n",
      "Test loss: 1.8118077516555786\n",
      "Test accuracy: 0.7710462212562561\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 270ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "CARDIOMEGALY       0.84      0.78      0.81       685\n",
      "       COVID       0.78      0.66      0.71       685\n",
      "      NORMAL       0.77      0.76      0.77       685\n",
      "   PNEUMONIA       0.89      0.88      0.88       685\n",
      "PNEUMOTHORAX       0.54      0.73      0.62       685\n",
      "TUBERCULOSIS       0.91      0.82      0.86       685\n",
      "\n",
      "    accuracy                           0.77      4110\n",
      "   macro avg       0.79      0.77      0.78      4110\n",
      "weighted avg       0.79      0.77      0.78      4110\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[533  33   3   4 109   3]\n",
      " [ 34 451  35   6 127  32]\n",
      " [ 13  11 521  51  88   1]\n",
      " [  4  14  36 600  24   7]\n",
      " [ 33  50  75  14 503  10]\n",
      " [ 20  21   7   0  76 561]]\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "model = load_model('lung_detection_grunot30.h5')\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "print(f\"Test loss: {loss}\")\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "\n",
    "# Get the raw predictions for the test data\n",
    "y_pred_probs = model.predict(X_test_reshaped)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Get the class labels\n",
    "labels = test_generator.class_indices\n",
    "\n",
    "# Generate the classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "# Generate the confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "382899fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16050 images belonging to 6 classes.\n",
      "Epoch 1/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 604ms/step - accuracy: 0.4837 - loss: 1.3135\n",
      "Epoch 2/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 453ms/step - accuracy: 0.6736 - loss: 0.8835\n",
      "Epoch 3/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 567ms/step - accuracy: 0.7309 - loss: 0.7291\n",
      "Epoch 4/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 340ms/step - accuracy: 0.7580 - loss: 0.6708\n",
      "Epoch 5/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 574ms/step - accuracy: 0.8311 - loss: 0.4779\n",
      "Epoch 6/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 337ms/step - accuracy: 0.8231 - loss: 0.4838\n",
      "Epoch 7/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 558ms/step - accuracy: 0.8853 - loss: 0.3233\n",
      "Epoch 8/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 339ms/step - accuracy: 0.8772 - loss: 0.3332\n",
      "Epoch 9/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 557ms/step - accuracy: 0.9253 - loss: 0.2171\n",
      "Epoch 10/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 338ms/step - accuracy: 0.9156 - loss: 0.2317\n",
      "Epoch 11/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 561ms/step - accuracy: 0.9504 - loss: 0.1448\n",
      "Epoch 12/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 337ms/step - accuracy: 0.9398 - loss: 0.1707\n",
      "Epoch 13/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 558ms/step - accuracy: 0.9624 - loss: 0.1129\n",
      "Epoch 14/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 338ms/step - accuracy: 0.9485 - loss: 0.1427\n",
      "Epoch 15/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 557ms/step - accuracy: 0.9727 - loss: 0.0853\n",
      "Epoch 16/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 338ms/step - accuracy: 0.9611 - loss: 0.1117\n",
      "Epoch 17/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 558ms/step - accuracy: 0.9828 - loss: 0.0557\n",
      "Epoch 18/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 339ms/step - accuracy: 0.9723 - loss: 0.0855\n",
      "Epoch 19/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 563ms/step - accuracy: 0.9840 - loss: 0.0497\n",
      "Epoch 20/20\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 341ms/step - accuracy: 0.9776 - loss: 0.0600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4110 images belonging to 6 classes.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "array() got an unexpected keyword argument 'dtasktype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 97\u001b[0m\n\u001b[0;32m     94\u001b[0m         y_test\u001b[38;5;241m.\u001b[39mappend(labels[j])\n\u001b[0;32m     96\u001b[0m X_test_reshaped \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_test_reshaped)\n\u001b[1;32m---> 97\u001b[0m y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_test, dtasktype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Add a time dimension to the input data\u001b[39;00m\n\u001b[0;32m    100\u001b[0m X_test_reshaped \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(X_test_reshaped, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: array() got an unexpected keyword argument 'dtasktype'"
     ]
    }
   ],
   "source": [
    "    import os\n",
    "    import numpy as np\n",
    "    from tensorflow.keras.models import Sequential, load_model\n",
    "    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GRU, TimeDistributed\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "    # Set the path to your dataset\n",
    "    train_data_dir = r\"D:\\new smote\\train-20230326T152931Z-001\\train\"\n",
    "\n",
    "    # Specify image dimensions and batch size\n",
    "    img_width, img_height = 150, 150\n",
    "    batch_size = 32\n",
    "\n",
    "    # Extract class names from the folder names\n",
    "    class_names = sorted(os.listdir(train_data_dir))\n",
    "\n",
    "    # Create data generator for training data\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='sparse',\n",
    "        shuffle=False)  # Disable shuffling to maintain the order of images\n",
    "\n",
    "    # Reshape the input data to include a time dimension\n",
    "    X_train_reshaped = []\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(len(train_generator)):\n",
    "        images, labels = train_generator[i]\n",
    "        for j in range(len(images)):\n",
    "            X_train_reshaped.append(images[j])\n",
    "            y_train.append(labels[j])\n",
    "\n",
    "    X_train_reshaped = np.array(X_train_reshaped)\n",
    "    y_train = np.array(y_train, dtype=np.int32)\n",
    "\n",
    "    # Add a time dimension to the input data\n",
    "    X_train_reshaped = np.expand_dims(X_train_reshaped, axis=1)\n",
    "\n",
    "    # Define the GRU model\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(1, img_width, img_height, 3)))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "    model.add(TimeDistributed(Conv2D(128, (3, 3), activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(GRU(128, activation='relu', return_sequences=False))\n",
    "    model.add(Dense(len(class_names), activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train_reshaped,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=20,\n",
    "        steps_per_epoch=312)\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save('lung_detection_grunot20.h5')\n",
    "\n",
    "    # Set the path to your test dataset\n",
    "    test_data_dir = r\"D:\\new smote\\test-20230326T155708Z-001\\test\"\n",
    "\n",
    "    # Create data generator for test data\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='sparse',\n",
    "        shuffle=False)  # Disable shuffling to maintain the order of images\n",
    "\n",
    "    # Reshape the input data to include a time dimension\n",
    "    X_test_reshaped = []\n",
    "    y_test = []\n",
    "\n",
    "    for i in range(len(test_generator)):\n",
    "        images, labels = test_generator[i]\n",
    "        for j in range(len(images)):\n",
    "            X_test_reshaped.append(images[j])\n",
    "            y_test.append(labels[j])\n",
    "\n",
    "    X_test_reshaped = np.array(X_test_reshaped)\n",
    "    y_test = np.array(y_test, dtasktype=np.int32)\n",
    "\n",
    "    # Add a time dimension to the input data\n",
    "    X_test_reshaped = np.expand_dims(X_test_reshaped, axis=1)\n",
    "\n",
    "    # Load the trained model\n",
    "    model = load_model('lung_detection_grunot20.h5')\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "    print(f\"Test loss: {loss}\")\n",
    "    print(f\"Test accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "637b10a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4110 images belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 115ms/step - accuracy: 0.8092 - loss: 1.2073\n",
      "Test loss: 1.155372142791748\n",
      "Test accuracy: 0.803892970085144\n"
     ]
    }
   ],
   "source": [
    "    # Set the path to your test dataset\n",
    "    test_data_dir = r\"D:\\new smote\\test-20230326T155708Z-001\\test\"\n",
    "\n",
    "    # Create data generator for test data\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='sparse',\n",
    "        shuffle=False)  # Disable shuffling to maintain the order of images\n",
    "\n",
    "    # Reshape the input data to include a time dimension\n",
    "    X_test_reshaped = []\n",
    "    y_test = []\n",
    "\n",
    "    for i in range(len(test_generator)):\n",
    "        images, labels = test_generator[i]\n",
    "        for j in range(len(images)):\n",
    "            X_test_reshaped.append(images[j])\n",
    "            y_test.append(labels[j])\n",
    "\n",
    "    X_test_reshaped = np.array(X_test_reshaped)\n",
    "    y_test = np.array(y_test, dtype=np.int32)\n",
    "\n",
    "    # Add a time dimension to the input data\n",
    "    X_test_reshaped = np.expand_dims(X_test_reshaped, axis=1)\n",
    "\n",
    "    # Load the trained model\n",
    "    model = load_model('lung_detection_grunot20.h5')\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "    print(f\"Test loss: {loss}\")\n",
    "    print(f\"Test accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61cda13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 114ms/step - accuracy: 0.8092 - loss: 1.2073\n",
      "Test loss: 1.155372142791748\n",
      "Test accuracy: 0.803892970085144\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 115ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "CARDIOMEGALY       0.85      0.81      0.83       685\n",
      "       COVID       0.75      0.75      0.75       685\n",
      "      NORMAL       0.77      0.80      0.79       685\n",
      "   PNEUMONIA       0.94      0.87      0.90       685\n",
      "PNEUMOTHORAX       0.65      0.69      0.67       685\n",
      "TUBERCULOSIS       0.89      0.90      0.90       685\n",
      "\n",
      "    accuracy                           0.80      4110\n",
      "   macro avg       0.81      0.80      0.81      4110\n",
      "weighted avg       0.81      0.80      0.81      4110\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[554  47   1   2  74   7]\n",
      " [ 34 514  31   4  71  31]\n",
      " [ 18  14 550  28  64  11]\n",
      " [  7  14  49 598  13   4]\n",
      " [ 34  72  80   7 471  21]\n",
      " [  5  24   4   0  35 617]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Load the trained model\n",
    "model = load_model('lung_detection_grunot20.h5')\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "print(f\"Test loss: {loss}\")\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "\n",
    "# Get the raw predictions for the test data\n",
    "y_pred_probs = model.predict(X_test_reshaped)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Get the class labels\n",
    "labels = test_generator.class_indices\n",
    "\n",
    "# Generate the classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "# Generate the confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eb8b531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16050 images belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 576ms/step - accuracy: 0.4789 - loss: 1.3109\n",
      "Epoch 2/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 348ms/step - accuracy: 0.6986 - loss: 0.8362\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 584ms/step - accuracy: 0.7483 - loss: 0.6669\n",
      "Epoch 4/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 413ms/step - accuracy: 0.7845 - loss: 0.6047\n",
      "Epoch 5/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 785ms/step - accuracy: 0.8419 - loss: 0.4258\n",
      "Epoch 6/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 626ms/step - accuracy: 0.8484 - loss: 0.4220\n",
      "Epoch 7/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.9005 - loss: 0.2784\n",
      "Epoch 8/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 635ms/step - accuracy: 0.8891 - loss: 0.2935\n",
      "Epoch 9/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 1s/step - accuracy: 0.9282 - loss: 0.1931\n",
      "Epoch 10/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 624ms/step - accuracy: 0.9281 - loss: 0.2034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4110 images belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 219ms/step - accuracy: 0.7240 - loss: 1.1078\n",
      "Test loss: 0.9522036910057068\n",
      "Test accuracy: 0.7481752038002014\n"
     ]
    }
   ],
   "source": [
    "    import os\n",
    "    import numpy as np\n",
    "    from tensorflow.keras.models import Sequential, load_model\n",
    "    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GRU, TimeDistributed\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "    # Set the path to your dataset\n",
    "    train_data_dir = r\"D:\\new smote\\train-20230326T152931Z-001\\train\"\n",
    "\n",
    "    # Specify image dimensions and batch size\n",
    "    img_width, img_height = 150, 150\n",
    "    batch_size = 32\n",
    "\n",
    "    # Extract class names from the folder names\n",
    "    class_names = sorted(os.listdir(train_data_dir))\n",
    "\n",
    "    # Create data generator for training data\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='sparse',\n",
    "        shuffle=False)  # Disable shuffling to maintain the order of images\n",
    "\n",
    "    # Reshape the input data to include a time dimension\n",
    "    X_train_reshaped = []\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(len(train_generator)):\n",
    "        images, labels = train_generator[i]\n",
    "        for j in range(len(images)):\n",
    "            X_train_reshaped.append(images[j])\n",
    "            y_train.append(labels[j])\n",
    "\n",
    "    X_train_reshaped = np.array(X_train_reshaped)\n",
    "    y_train = np.array(y_train, dtype=np.int32)\n",
    "\n",
    "    # Add a time dimension to the input data\n",
    "    X_train_reshaped = np.expand_dims(X_train_reshaped, axis=1)\n",
    "\n",
    "    # Define the GRU model\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(1, img_width, img_height, 3)))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "    model.add(TimeDistributed(Conv2D(128, (3, 3), activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(GRU(128, activation='relu', return_sequences=False))\n",
    "    model.add(Dense(len(class_names), activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train_reshaped,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=10,\n",
    "        steps_per_epoch=312)\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save('lung_detection_grunot10.h5')\n",
    "\n",
    "    # Set the path to your test dataset\n",
    "    test_data_dir = r\"D:\\new smote\\test-20230326T155708Z-001\\test\"\n",
    "\n",
    "    # Create data generator for test data\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='sparse',\n",
    "        shuffle=False)  # Disable shuffling to maintain the order of images\n",
    "\n",
    "    # Reshape the input data to include a time dimension\n",
    "    X_test_reshaped = []\n",
    "    y_test = []\n",
    "\n",
    "    for i in range(len(test_generator)):\n",
    "        images, labels = test_generator[i]\n",
    "        for j in range(len(images)):\n",
    "            X_test_reshaped.append(images[j])\n",
    "            y_test.append(labels[j])\n",
    "\n",
    "    X_test_reshaped = np.array(X_test_reshaped)\n",
    "    y_test = np.array(y_test, dtype=np.int32)\n",
    "\n",
    "    # Add a time dimension to the input data\n",
    "    X_test_reshaped = np.expand_dims(X_test_reshaped, axis=1)\n",
    "\n",
    "    # Load the trained model\n",
    "    model = load_model('lung_detection_grunot10.h5')\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "    print(f\"Test loss: {loss}\")\n",
    "    print(f\"Test accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d1b7233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 191ms/step - accuracy: 0.7240 - loss: 1.1078\n",
      "Test loss: 0.9522036910057068\n",
      "Test accuracy: 0.7481752038002014\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 173ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "CARDIOMEGALY       0.89      0.66      0.76       685\n",
      "       COVID       0.72      0.70      0.71       685\n",
      "      NORMAL       0.85      0.62      0.71       685\n",
      "   PNEUMONIA       0.90      0.90      0.90       685\n",
      "PNEUMOTHORAX       0.48      0.83      0.61       685\n",
      "TUBERCULOSIS       0.93      0.78      0.85       685\n",
      "\n",
      "    accuracy                           0.75      4110\n",
      "   macro avg       0.80      0.75      0.76      4110\n",
      "weighted avg       0.80      0.75      0.76      4110\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[455  63   0   3 162   2]\n",
      " [ 24 479  10  11 143  18]\n",
      " [ 12  11 422  47 189   4]\n",
      " [  1  16  25 617  25   1]\n",
      " [ 10  51  37   4 571  12]\n",
      " [ 11  44   3   3  93 531]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Load the trained model\n",
    "model = load_model('lung_detection_grunot10.h5')\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "print(f\"Test loss: {loss}\")\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "\n",
    "# Get the raw predictions for the test data\n",
    "y_pred_probs = model.predict(X_test_reshaped)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Get the class labels\n",
    "labels = test_generator.class_indices\n",
    "\n",
    "# Generate the classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "# Generate the confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f70261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
