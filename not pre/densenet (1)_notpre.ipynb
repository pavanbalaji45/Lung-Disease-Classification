{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bae735d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 423ms/step\n",
      "Predicted class: COVID\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load the saved model\n",
    "model_path = 'lung_detection_dblstm.h5'\n",
    "loaded_model = load_model(model_path)\n",
    "\n",
    "# Define your custom class names\n",
    "class_names = ['CARDIOMEGALY', 'COVID', 'NORMAL', 'PNEUMONIA', 'PNEUMOTHORAX', 'TUBERCULOSIS']\n",
    "\n",
    "# Path to the image you want to predict\n",
    "image_path = r\"C:\\Users\\asuto\\Desktop\\intern ship 2\\lung detection\\archive\\test-20230326T155708Z-001\\test\\COVID\\COVID-19 (8).jpeg\"\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = image.load_img(image_path, target_size=(150, 150))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Add a batch dimension\n",
    "img_array = np.expand_dims(img_array, axis=1)  # Add a time dimension\n",
    "img_array /= 255.0  # Rescale pixel values to [0, 1]\n",
    "\n",
    "# Make a prediction\n",
    "predictions = loaded_model.predict(img_array)\n",
    "predicted_class = np.argmax(predictions)\n",
    "\n",
    "# Get the predicted class name from the custom class names list\n",
    "predicted_class_name = class_names[predicted_class]\n",
    "\n",
    "# Display the predicted class\n",
    "print(f\"Predicted class: {predicted_class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddfeef9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16050 images belonging to 6 classes.\n",
      "Found 4104 images belonging to 6 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3580s\u001b[0m 16s/step - accuracy: 0.5432 - loss: 1.6773 - val_accuracy: 0.7393 - val_loss: 0.7135\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1954s\u001b[0m 10s/step - accuracy: 0.7592 - loss: 0.6655 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1673s\u001b[0m 8s/step - accuracy: 0.7828 - loss: 0.6067 - val_accuracy: 0.7476 - val_loss: 0.6722\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1245s\u001b[0m 6s/step - accuracy: 0.7915 - loss: 0.5851 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1454s\u001b[0m 7s/step - accuracy: 0.7838 - loss: 0.5791 - val_accuracy: 0.8009 - val_loss: 0.5380\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m575s\u001b[0m 3s/step - accuracy: 0.8123 - loss: 0.4984 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1940s\u001b[0m 10s/step - accuracy: 0.8165 - loss: 0.5048 - val_accuracy: 0.7953 - val_loss: 0.5390\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m630s\u001b[0m 3s/step - accuracy: 0.7978 - loss: 0.5426 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m678s\u001b[0m 3s/step - accuracy: 0.8383 - loss: 0.4660 - val_accuracy: 0.7968 - val_loss: 0.5678\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m598s\u001b[0m 3s/step - accuracy: 0.8257 - loss: 0.4593 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Found 4110 images belonging to 6 classes.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 3s/step - accuracy: 0.8069 - loss: 0.5113\n",
      "Test Accuracy: 0.805596113204956\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import DenseNet121, InceptionV3\n",
    "\n",
    "# Define ensemble model using DenseNet121 and InceptionV3\n",
    "def ensemble_model(input_shape=(224, 224, 3), num_classes=6):\n",
    "    # Load pre-trained DenseNet121 model\n",
    "    densenet_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in densenet_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Load pre-trained InceptionV3 model\n",
    "    inception_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in inception_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Define input layer\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Extract features using DenseNet121\n",
    "    densenet_features = densenet_model(input_layer)\n",
    "    densenet_features = layers.GlobalAveragePooling2D()(densenet_features)\n",
    "\n",
    "    # Extract features using InceptionV3\n",
    "    inception_features = inception_model(input_layer)\n",
    "    inception_features = layers.GlobalAveragePooling2D()(inception_features)\n",
    "\n",
    "    # Concatenate features\n",
    "    merged_features = layers.Concatenate()([densenet_features, inception_features])\n",
    "\n",
    "    # Add a fully connected layer\n",
    "    merged_features = layers.Dense(512, activation='relu')(merged_features)\n",
    "\n",
    "    # Output layer\n",
    "    output = layers.Dense(num_classes, activation='softmax')(merged_features)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define data directories\n",
    "train_dir = r'D:\\new smote\\train-20230326T152931Z-001\\train'\n",
    "val_dir = r'D:\\new smote\\val-20230326T152503Z-001\\val'\n",
    "test_dir = r'D:\\new smote\\test-20230326T155708Z-001\\test'\n",
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Create data generators for training, validation, and testing\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "# Load ensemble model\n",
    "model = ensemble_model(input_shape=image_size + (3,), num_classes=num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=200,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_generator)\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "000d529c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 3s/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "CARDIOMEGALY       0.16      0.18      0.17       685\n",
      "       COVID       0.16      0.18      0.17       685\n",
      "      NORMAL       0.18      0.17      0.18       685\n",
      "   PNEUMONIA       0.16      0.18      0.17       685\n",
      "PNEUMOTHORAX       0.17      0.11      0.13       685\n",
      "TUBERCULOSIS       0.17      0.18      0.18       685\n",
      "\n",
      "    accuracy                           0.17      4110\n",
      "   macro avg       0.17      0.17      0.17      4110\n",
      "weighted avg       0.17      0.17      0.17      4110\n",
      "\n",
      "Confusion Matrix:\n",
      "[[123 118 100 141  90 113]\n",
      " [134 126 116 108  63 138]\n",
      " [117 126 116 131  79 116]\n",
      " [134 123 110 122  74 122]\n",
      " [126 138  99 129  76 117]\n",
      " [133 138  91 129  70 124]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_probabilities = model.predict(test_generator)\n",
    "y_pred = np.argmax(y_pred_probabilities, axis=1)\n",
    "\n",
    "# Convert true labels to class indices\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys())\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ab5e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16050 images belonging to 6 classes.\n",
      "Found 4104 images belonging to 6 classes.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1391s\u001b[0m 7s/step - accuracy: 0.5479 - loss: 1.6399 - val_accuracy: 0.7191 - val_loss: 0.7464\n",
      "Epoch 2/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m875s\u001b[0m 4s/step - accuracy: 0.7508 - loss: 0.6683 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1206s\u001b[0m 6s/step - accuracy: 0.7955 - loss: 0.5630 - val_accuracy: 0.7943 - val_loss: 0.5794\n",
      "Epoch 4/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1192s\u001b[0m 6s/step - accuracy: 0.7931 - loss: 0.5685 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 5/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1087s\u001b[0m 5s/step - accuracy: 0.7952 - loss: 0.5566 - val_accuracy: 0.8095 - val_loss: 0.5238\n",
      "Epoch 6/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 2s/step - accuracy: 0.8268 - loss: 0.4839 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 7/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m961s\u001b[0m 5s/step - accuracy: 0.8088 - loss: 0.5244 - val_accuracy: 0.7956 - val_loss: 0.5786\n",
      "Epoch 8/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m600s\u001b[0m 3s/step - accuracy: 0.8110 - loss: 0.5221 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 9/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m673s\u001b[0m 3s/step - accuracy: 0.8234 - loss: 0.4847 - val_accuracy: 0.8197 - val_loss: 0.4925\n",
      "Epoch 10/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m596s\u001b[0m 3s/step - accuracy: 0.8321 - loss: 0.4549 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 11/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m962s\u001b[0m 5s/step - accuracy: 0.8321 - loss: 0.4557 - val_accuracy: 0.8192 - val_loss: 0.4705\n",
      "Epoch 12/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 2s/step - accuracy: 0.8401 - loss: 0.4277 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 13/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m966s\u001b[0m 5s/step - accuracy: 0.8394 - loss: 0.4342 - val_accuracy: 0.8143 - val_loss: 0.4858\n",
      "Epoch 14/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m593s\u001b[0m 3s/step - accuracy: 0.8261 - loss: 0.4600 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 15/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m671s\u001b[0m 3s/step - accuracy: 0.8396 - loss: 0.4459 - val_accuracy: 0.8294 - val_loss: 0.4585\n",
      "Epoch 16/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m594s\u001b[0m 3s/step - accuracy: 0.8411 - loss: 0.4377 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 17/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m964s\u001b[0m 5s/step - accuracy: 0.8440 - loss: 0.4162 - val_accuracy: 0.8370 - val_loss: 0.4428\n",
      "Epoch 18/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 1s/step - accuracy: 0.8544 - loss: 0.4009 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 19/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m963s\u001b[0m 5s/step - accuracy: 0.8443 - loss: 0.4194 - val_accuracy: 0.8009 - val_loss: 0.5080\n",
      "Epoch 20/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m591s\u001b[0m 3s/step - accuracy: 0.8453 - loss: 0.4068 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Found 4110 images belonging to 6 classes.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 3s/step - accuracy: 0.8287 - loss: 0.4613\n",
      "Test Accuracy: 0.8282238245010376\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import DenseNet121, InceptionV3\n",
    "\n",
    "# Define ensemble model using DenseNet121 and InceptionV3\n",
    "def ensemble_model(input_shape=(224, 224, 3), num_classes=6):\n",
    "    # Load pre-trained DenseNet121 model\n",
    "    densenet_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in densenet_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Load pre-trained InceptionV3 model\n",
    "    inception_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in inception_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Define input layer\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Extract features using DenseNet121\n",
    "    densenet_features = densenet_model(input_layer)\n",
    "    densenet_features = layers.GlobalAveragePooling2D()(densenet_features)\n",
    "\n",
    "    # Extract features using InceptionV3\n",
    "    inception_features = inception_model(input_layer)\n",
    "    inception_features = layers.GlobalAveragePooling2D()(inception_features)\n",
    "\n",
    "    # Concatenate features\n",
    "    merged_features = layers.Concatenate()([densenet_features, inception_features])\n",
    "\n",
    "    # Add a fully connected layer\n",
    "    merged_features = layers.Dense(512, activation='relu')(merged_features)\n",
    "\n",
    "    # Output layer\n",
    "    output = layers.Dense(num_classes, activation='softmax')(merged_features)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define data directories\n",
    "train_dir = r'D:\\new smote\\train-20230326T152931Z-001\\train'\n",
    "val_dir = r'D:\\new smote\\val-20230326T152503Z-001\\val'\n",
    "test_dir = r'D:\\new smote\\test-20230326T155708Z-001\\test'\n",
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Create data generators for training, validation, and testing\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "# Load ensemble model\n",
    "model = ensemble_model(input_shape=image_size + (3,), num_classes=num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=200,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_generator)\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcf7011f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 3s/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "CARDIOMEGALY       0.18      0.13      0.16       685\n",
      "       COVID       0.18      0.20      0.19       685\n",
      "      NORMAL       0.17      0.18      0.18       685\n",
      "   PNEUMONIA       0.15      0.14      0.14       685\n",
      "PNEUMOTHORAX       0.17      0.16      0.17       685\n",
      "TUBERCULOSIS       0.16      0.19      0.17       685\n",
      "\n",
      "    accuracy                           0.17      4110\n",
      "   macro avg       0.17      0.17      0.17      4110\n",
      "weighted avg       0.17      0.17      0.17      4110\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 92 115 113 124 108 133]\n",
      " [ 79 137 134 106  98 131]\n",
      " [ 76 120 125 104 125 135]\n",
      " [ 72 121 126  95 118 153]\n",
      " [ 96 130 110 101 112 136]\n",
      " [ 85 120 124 122 105 129]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_probabilities = model.predict(test_generator)\n",
    "y_pred = np.argmax(y_pred_probabilities, axis=1)\n",
    "\n",
    "# Convert true labels to class indices\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys())\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1337642f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16050 images belonging to 6 classes.\n",
      "Found 4104 images belonging to 6 classes.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1023s\u001b[0m 5s/step - accuracy: 0.5381 - loss: 1.6906 - val_accuracy: 0.7222 - val_loss: 0.7240\n",
      "Epoch 2/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 3s/step - accuracy: 0.7593 - loss: 0.6744 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m697s\u001b[0m 3s/step - accuracy: 0.7748 - loss: 0.6125 - val_accuracy: 0.7364 - val_loss: 0.7132\n",
      "Epoch 4/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 3s/step - accuracy: 0.7916 - loss: 0.5703 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 5/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1001s\u001b[0m 5s/step - accuracy: 0.7912 - loss: 0.5630 - val_accuracy: 0.7018 - val_loss: 0.7822\n",
      "Epoch 6/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 2s/step - accuracy: 0.7898 - loss: 0.5731 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 7/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1000s\u001b[0m 5s/step - accuracy: 0.8226 - loss: 0.4982 - val_accuracy: 0.8185 - val_loss: 0.5025\n",
      "Epoch 8/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 3s/step - accuracy: 0.8266 - loss: 0.4847 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 9/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m693s\u001b[0m 3s/step - accuracy: 0.8228 - loss: 0.4915 - val_accuracy: 0.7963 - val_loss: 0.5445\n",
      "Epoch 10/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 3s/step - accuracy: 0.8409 - loss: 0.4505 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 11/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1009s\u001b[0m 5s/step - accuracy: 0.8277 - loss: 0.4678 - val_accuracy: 0.7943 - val_loss: 0.6029\n",
      "Epoch 12/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 2s/step - accuracy: 0.8254 - loss: 0.4737 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 13/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1001s\u001b[0m 5s/step - accuracy: 0.8399 - loss: 0.4385 - val_accuracy: 0.8338 - val_loss: 0.4736\n",
      "Epoch 14/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 3s/step - accuracy: 0.8389 - loss: 0.4398 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 15/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m695s\u001b[0m 3s/step - accuracy: 0.8484 - loss: 0.4249 - val_accuracy: 0.8484 - val_loss: 0.4210\n",
      "Epoch 16/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m616s\u001b[0m 3s/step - accuracy: 0.8545 - loss: 0.4041 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 17/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m996s\u001b[0m 5s/step - accuracy: 0.8381 - loss: 0.4358 - val_accuracy: 0.7919 - val_loss: 0.5308\n",
      "Epoch 18/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 2s/step - accuracy: 0.8505 - loss: 0.4006 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 19/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m997s\u001b[0m 5s/step - accuracy: 0.8328 - loss: 0.4456 - val_accuracy: 0.8319 - val_loss: 0.4483\n",
      "Epoch 20/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 3s/step - accuracy: 0.8582 - loss: 0.4090 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 21/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m693s\u001b[0m 3s/step - accuracy: 0.8359 - loss: 0.4388 - val_accuracy: 0.8326 - val_loss: 0.4510\n",
      "Epoch 22/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 3s/step - accuracy: 0.8548 - loss: 0.4006 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 23/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m992s\u001b[0m 5s/step - accuracy: 0.8624 - loss: 0.3820 - val_accuracy: 0.8346 - val_loss: 0.4501\n",
      "Epoch 24/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 2s/step - accuracy: 0.8626 - loss: 0.3800 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 25/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m995s\u001b[0m 5s/step - accuracy: 0.8568 - loss: 0.3995 - val_accuracy: 0.8470 - val_loss: 0.4219\n",
      "Epoch 26/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m610s\u001b[0m 3s/step - accuracy: 0.8650 - loss: 0.3832 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 27/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m694s\u001b[0m 3s/step - accuracy: 0.8572 - loss: 0.4087 - val_accuracy: 0.8380 - val_loss: 0.4476\n",
      "Epoch 28/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 3s/step - accuracy: 0.8594 - loss: 0.3848 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 29/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m993s\u001b[0m 5s/step - accuracy: 0.8640 - loss: 0.3654 - val_accuracy: 0.8387 - val_loss: 0.4290\n",
      "Epoch 30/30\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 2s/step - accuracy: 0.8635 - loss: 0.3715 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Found 4110 images belonging to 6 classes.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 3s/step - accuracy: 0.8537 - loss: 0.4109\n",
      "Test Accuracy: 0.8496350646018982\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import DenseNet121, InceptionV3\n",
    "\n",
    "# Define ensemble model using DenseNet121 and InceptionV3\n",
    "def ensemble_model(input_shape=(224, 224, 3), num_classes=6):\n",
    "    # Load pre-trained DenseNet121 model\n",
    "    densenet_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in densenet_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Load pre-trained InceptionV3 model\n",
    "    inception_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in inception_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Define input layer\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Extract features using DenseNet121\n",
    "    densenet_features = densenet_model(input_layer)\n",
    "    densenet_features = layers.GlobalAveragePooling2D()(densenet_features)\n",
    "\n",
    "    # Extract features using InceptionV3\n",
    "    inception_features = inception_model(input_layer)\n",
    "    inception_features = layers.GlobalAveragePooling2D()(inception_features)\n",
    "\n",
    "    # Concatenate features\n",
    "    merged_features = layers.Concatenate()([densenet_features, inception_features])\n",
    "\n",
    "    # Add a fully connected layer\n",
    "    merged_features = layers.Dense(512, activation='relu')(merged_features)\n",
    "\n",
    "    # Output layer\n",
    "    output = layers.Dense(num_classes, activation='softmax')(merged_features)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define data directories\n",
    "train_dir = r'D:\\new smote\\train-20230326T152931Z-001\\train'\n",
    "val_dir = r'D:\\new smote\\val-20230326T152503Z-001\\val'\n",
    "test_dir = r'D:\\new smote\\test-20230326T155708Z-001\\test'\n",
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Create data generators for training, validation, and testing\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "# Load ensemble model\n",
    "model = ensemble_model(input_shape=image_size + (3,), num_classes=num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=200,\n",
    "    epochs=30,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_generator)\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b76da5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 3s/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "CARDIOMEGALY       0.16      0.17      0.17       685\n",
      "       COVID       0.17      0.15      0.16       685\n",
      "      NORMAL       0.16      0.14      0.15       685\n",
      "   PNEUMONIA       0.19      0.18      0.18       685\n",
      "PNEUMOTHORAX       0.19      0.24      0.21       685\n",
      "TUBERCULOSIS       0.15      0.15      0.15       685\n",
      "\n",
      "    accuracy                           0.17      4110\n",
      "   macro avg       0.17      0.17      0.17      4110\n",
      "weighted avg       0.17      0.17      0.17      4110\n",
      "\n",
      "Confusion Matrix:\n",
      "[[116  94 111 115 141 108]\n",
      " [111 101  88 105 150 130]\n",
      " [139  97  97 111 123 118]\n",
      " [112 114  89 122 144 104]\n",
      " [117  89  97  99 161 122]\n",
      " [123 106 122 105 128 101]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_probabilities = model.predict(test_generator)\n",
    "y_pred = np.argmax(y_pred_probabilities, axis=1)\n",
    "\n",
    "# Convert true labels to class indices\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys())\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe470bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
